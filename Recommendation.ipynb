{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basics of Recommendation Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial.distance import correlation, cosine\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.asarray([[4,3,2,3], [1,2,3,1], [np.nan,2,1,np.nan], [4,3,2,np.nan]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def cosine_similarity(v1,v2, metric='cosine'):\n",
    "    if metric == 'correlation':\n",
    "        v1 = v1 - np.nanmean(v1)\n",
    "        v2 = v2 - np.nanmean(v2)\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        if np.isnan(x) or np.isnan(y): continue\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy/math.sqrt(sumxx*sumyy)\n",
    "\n",
    "def sim_matrix(M, dimension='user', metric='cosine'):\n",
    "    N = M.shape[0] if dimension == 'user' else M.shape[1]\n",
    "    sim = np.zeros([N,N])\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if i == j:\n",
    "                sim[i,j] = 0\n",
    "                continue\n",
    "            if dimension == 'user':\n",
    "                v1, v2 = M[i,:], M[j,:]\n",
    "            else:\n",
    "                v1, v2 = M[:,i], M[:,j]\n",
    "            sim[i][j] = cosine_similarity(v1,v2,metric)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9922778767136677"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(M[0,:], M[2,:], 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.79582243, 0.99227788, 1.        ],\n",
       "       [0.79582243, 0.        , 0.86824314, 0.79406667],\n",
       "       [0.99227788, 0.86824314, 0.        , 0.99227788],\n",
       "       [1.        , 0.79406667, 0.99227788, 0.        ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix(M, 'user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.9649505 , 0.80218063, 0.99705449],\n",
       "       [0.9649505 , 0.        , 0.92450033, 0.96476382],\n",
       "       [0.80218063, 0.92450033, 0.        , 0.78935222],\n",
       "       [0.99705449, 0.96476382, 0.78935222, 0.        ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix(M, 'item')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071067811865475"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(M[0,:], M[2,:], 'correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -0.85280287,  0.70710678,  1.        ],\n",
       "       [-0.85280287,  0.        , -0.5547002 , -0.95618289],\n",
       "       [ 0.70710678, -0.5547002 ,  0.        ,  0.70710678],\n",
       "       [ 1.        , -0.95618289,  0.70710678,  0.        ]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix(M, 'user', 'correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.94280904, -0.81649658,  0.9486833 ],\n",
       "       [ 0.94280904,  0.        ,  0.        ,  1.        ],\n",
       "       [-0.81649658,  0.        ,  0.        , -0.70710678],\n",
       "       [ 0.9486833 ,  1.        , -0.70710678,  0.        ]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix(M, 'item', 'correlation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Compute the missing rating in this table using user-based collaborative filtering (CF). (Use cosine similarity, then use Pearson similarity). Assume taking all neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_cf(M, metric='cosine'):\n",
    "    pred = np.copy(M)\n",
    "    n_users, n_items = M.shape\n",
    "    avg_ratings = np.nanmean(M, axis=1)\n",
    "    sim_users = sim_matrix(M, 'user', metric)\n",
    "    for i in range(n_users):\n",
    "        for j in range(n_items):\n",
    "            if np.isnan(M[i,j]):\n",
    "                #Get similarity values for current user\n",
    "                cur_user_row = sim_users[i]\n",
    "                highest = -99999\n",
    "                highest_index = -1\n",
    "                cur_index = 0\n",
    "                #Iterate over values to find most similar index\n",
    "                for x in cur_user_row:\n",
    "                    if(highest < x or highest_index == -1):\n",
    "                        highest = x;\n",
    "                        highest_index = cur_index\n",
    "                    cur_index += 1\n",
    "                #Get row of most similar user and take their rating for item at column 'j'\n",
    "                pred[i,j] = M[highest_index][j]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF (Cosine): \n",
      "     0    1    2    3\n",
      "0  4.0  3.0  2.0  3.0\n",
      "1  1.0  2.0  3.0  1.0\n",
      "2  4.0  2.0  1.0  3.0\n",
      "3  4.0  3.0  2.0  3.0\n",
      "User-based CF (Pearson): \n",
      "     0    1    2    3\n",
      "0  4.0  3.0  2.0  3.0\n",
      "1  1.0  2.0  3.0  1.0\n",
      "2  4.0  2.0  1.0  3.0\n",
      "3  4.0  3.0  2.0  3.0\n"
     ]
    }
   ],
   "source": [
    "print(\"User-based CF (Cosine): \\n\" + str(pd.DataFrame(user_cf(M, 'cosine'))))\n",
    "print(\"User-based CF (Pearson): \\n\" + str(pd.DataFrame(user_cf(M, 'correlation'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Similarly, computing the missing rating using item-based CF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_cf(M, metric='cosine'):\n",
    "    pred = np.copy(M)\n",
    "    n_users, n_items = M.shape\n",
    "    avg_ratings = np.nanmean(M, axis=0)\n",
    "    sim_items = sim_matrix(M, 'item', metric)\n",
    "    for i in range(n_users):\n",
    "        for j in range(n_items):\n",
    "            if np.isnan(M[i,j]):\n",
    "                #Get similarity values for current item\n",
    "                cur_item_row = sim_items[i]\n",
    "                highest = -99999\n",
    "                highest_index = -1\n",
    "                cur_index = 0\n",
    "                #Iterate over values to find most similar index\n",
    "                for x in cur_item_row:\n",
    "                    if(highest < x or highest_index == -1):\n",
    "                        highest = x;\n",
    "                        highest_index = cur_index\n",
    "                    cur_index += 1\n",
    "                #Get row of most similarly rated item and take their rating for user at column 'i'\n",
    "                pred[i,j] = M[i][highest_index]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item-based CF (Cosine): \n",
      "     0    1    2    3\n",
      "0  4.0  3.0  2.0  3.0\n",
      "1  1.0  2.0  3.0  1.0\n",
      "2  2.0  2.0  1.0  2.0\n",
      "3  4.0  3.0  2.0  4.0\n",
      "Item-based CF (Pearson): \n",
      "     0    1    2    3\n",
      "0  4.0  3.0  2.0  3.0\n",
      "1  1.0  2.0  3.0  1.0\n",
      "2  2.0  2.0  1.0  2.0\n",
      "3  4.0  3.0  2.0  3.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Item-based CF (Cosine): \\n\" + str(pd.DataFrame(item_cf(M, 'cosine'))))\n",
    "print(\"Item-based CF (Pearson): \\n\" + str(pd.DataFrame(item_cf(M, 'correlation'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluating Recommendation Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3\n",
       "0  4  3  2  3\n",
       "1  1  2  3  1\n",
       "2  1  2  1  2\n",
       "3  4  3  2  4"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_result = np.asarray([[4,3,2,3], \n",
    "                [1,2,3,1],\n",
    "                [1,2,1,2],\n",
    "                [4,3,2,4]])\n",
    "pd.DataFrame(M_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def evaluateRS(ratings, groundtruth, method='user_cf', metric='cosine'):\n",
    "    if method == 'user_cf':\n",
    "        prediction = user_cf(ratings, metric)\n",
    "    else:\n",
    "        prediction = item_cf(ratings, metric)\n",
    "    MSE = mean_squared_error(prediction, groundtruth)\n",
    "    RMSE = round(math.sqrt(MSE),3)\n",
    "    #print(\"RMSE using {0} approach ({2}) is: {1}\".format(method, RMSE, metric))\n",
    "    #print(pd.DataFrame(prediction))\n",
    "    return MSE, RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Mean Square Error) using USER CF, Cosine metric: 0.6875; (Root: 0.829)\n",
      "MSE (Mean Square Error) using ITEM CF, Cosine metric: 0.0625; (Root: 0.25)\n",
      "MSE (Mean Square Error) using USER CF, Correlation metric: 0.6875; (Root: 0.829)\n",
      "MSE (Mean Square Error) using ITEM CF, Correlation metric: 0.125; (Root: 0.354)\n"
     ]
    }
   ],
   "source": [
    "mse, rmse = evaluateRS(M, M_result, method='user_cf', metric='cosine')\n",
    "print(\"MSE (Mean Square Error) using USER CF, Cosine metric: \"+str(mse)+\"; (Root: \"+str(rmse)+\")\")\n",
    "\n",
    "mse, rmse = evaluateRS(M, M_result, method='item_cf', metric='cosine')\n",
    "print(\"MSE (Mean Square Error) using ITEM CF, Cosine metric: \"+str(mse)+\"; (Root: \"+str(rmse)+\")\")\n",
    "\n",
    "mse, rmse = evaluateRS(M, M_result, method='user_cf', metric='correlation')\n",
    "print(\"MSE (Mean Square Error) using USER CF, Correlation metric: \"+str(mse)+\"; (Root: \"+str(rmse)+\")\")\n",
    "\n",
    "mse, rmse = evaluateRS(M, M_result, method='item_cf', metric='correlation')\n",
    "print(\"MSE (Mean Square Error) using ITEM CF, Correlation metric: \"+str(mse)+\"; (Root: \"+str(rmse)+\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "The lowest value given by this evaluation method is Item CF using the cosine metric. This indicates a better fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kendall's Tau Ranking Accuracy Score (USER CF, cosine) = 0.7\n",
      "Kendall's Tau Ranking Accuracy Score (ITEM CF, cosine) = 0.8943375672974064\n",
      "Kendall's Tau Ranking Accuracy Score (USER CF, correlation) = 0.7\n",
      "Kendall's Tau Ranking Accuracy Score (ITEM CF, correlation) = 0.8443375672974064\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def evaluate_rank(ratings, groundtruth, method='user_cf', metric='cosine'):\n",
    "    if method == 'user_cf':\n",
    "        prediction = user_cf(ratings, metric)\n",
    "    else:\n",
    "        prediction = item_cf(ratings, metric)\n",
    "    n_users = M.shape[0]\n",
    "    avg_tau = 0\n",
    "    for i in range(n_users):\n",
    "        tau, p_value = stats.kendalltau(M_result[i,:], prediction[i,:])\n",
    "        avg_tau += tau\n",
    "    avg_tau = avg_tau / n_users\n",
    "    #clear_output(wait=True)\n",
    "    return avg_tau\n",
    "\n",
    "print(\"Kendall's Tau Ranking Accuracy Score (USER CF, cosine) = \" + str(evaluate_rank(M, M_result, 'user_cf', 'cosine')))\n",
    "print(\"Kendall's Tau Ranking Accuracy Score (ITEM CF, cosine) = \" + str(evaluate_rank(M, M_result, 'item_cf', 'cosine')))\n",
    "print(\"Kendall's Tau Ranking Accuracy Score (USER CF, correlation) = \" + str(evaluate_rank(M, M_result, 'user_cf', 'correlation')))\n",
    "print(\"Kendall's Tau Ranking Accuracy Score (ITEM CF, correlation) = \" + str(evaluate_rank(M, M_result, 'item_cf', 'correlation')))\n",
    "\n",
    "#predicted_item_cf = item_cf(M, 'cosine')\n",
    "#predicted_user_cf = user_cf(M, 'cosine')\n",
    "#ground_truth = M_result\n",
    "\n",
    "#concordant_i = 0\n",
    "#discordant_i = 0\n",
    "\n",
    "#concordant_u = 0\n",
    "#discordant_u = 0\n",
    "\n",
    "#for i in range(0,ground_truth.shape[0]):\n",
    "#    s = 0\n",
    "#    for j in range(0,ground_truth.shape[1]):\n",
    "#        val_truth = ground_truth[i][j]\n",
    "#        val_pred_i = predicted_item_cf[i][j]\n",
    "#        val_pred_u = predicted_user_cf[i][j]\n",
    "#        if()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The accuracy ranking showed the best result using Item CF and the cosine metric. User CF remained unchanged between cosine and correlation metric types but most likely only due to lack of smaller decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
